{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Contrastive Learning</center>\n",
    "\n",
    "### <center>Using SimCLR to illustrate the principles of self-supervised contrastive learning (CL)</center>\n",
    "\n",
    "<center>Nick Glaser</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CL - Introduction\n",
    "\n",
    "- What's the need for CL?\n",
    "  - Leverage large amounts of unlabeled data\n",
    "  - Build _good_ transfer learning models\n",
    "    - [Cross-entropy-trained models generally have suboptimal transfer performance](https://arxiv.org/abs/2103.13517)\n",
    "    - [Transferability depends on many factors and is fragile in XEnt CV models](https://arxiv.org/abs/1411.1792)\n",
    "\n",
    "[//]: # \"Transferring between tasks is hard. Task similarity matters, data similarity matters. Classic classification & xent loss results in very semantically high-level features, e.g. petal-detectors - good for IRIS transfer, bad for medical images and planes. Can be combatted by using intermediate network layers instead of last but still run into issues, lower features might be too general, co-adapted neurons from original task cause instabilities, etc.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./media/transfer_learning.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./media/imagenet_v_cl.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can we frame a learning problem when we don't have labels for our data?\n",
    "  - Intuition: one-versus-rest/ instance-level classification [Wu et al. 2018](https://arxiv.org/abs/1805.01978v1)\n",
    "  - Learn embeddings that are similar for variations of the same sample while different for other examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formalize a problem statement\n",
    "  - Anchor $x$: Random sample drawn from data distribution\n",
    "  - Positive sample $x^+$: Semantically similar sample to anchor\n",
    "  - Negative sample $x^-$: Semantically disimilar sample to anchor\n",
    "  - Encoder function $f: x \\mapsto \\textbf{h}$\n",
    "  - **Goal**: learn $f$ such that $x$ and $x^+$ are closer in embedding space and $x$ and $x^-$ are far away, formally:\n",
    "\n",
    "**Contrastive learning seeks to satisfy**:\n",
    "$sim(\\textbf{h}, \\textbf{h}^+) > sim(\\textbf{h}, \\textbf{h}^-)$\n",
    "\n",
    "- Negative examples are easy to obtain for instance-level discrimination, but how do we get good positive examples?\n",
    "  - Pretext tasks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./media/augmentations.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98428c76795c4eee852acdc4989d84d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Data Transforms', index=(4, 1, 2, 3), options=('horizontal_flip', 'color_jitter', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some preprocessing code adapted from https://github.com/PyTorchLightning/lightning-bolts/\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.data_utils import util as data_util\n",
    "from torchvision import transforms\n",
    "\n",
    "# normalization stats for rgb channels\n",
    "cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
    "cifar100_std = (0.2675, 0.2565, 0.2761)\n",
    "cifar100_dim = 32\n",
    "\n",
    "# augmentations, can modify values\n",
    "normalize = transforms.Normalize(mean=cifar100_mean, std=cifar100_std)\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "color_jitter = transforms.RandomApply(\n",
    "    [transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8)\n",
    "grayscale = transforms.RandomGrayscale(p=0.2)\n",
    "gauss_blur = transforms.RandomApply([data_util.GaussianBlur2()], p=0.5)\n",
    "resized_crop = transforms.RandomResizedCrop(size=cifar100_dim)\n",
    "\n",
    "# allowed transforms\n",
    "transform_dict = {\n",
    "    'horizontal_flip': horizontal_flip,\n",
    "    'color_jitter': color_jitter,\n",
    "    'grayscale': grayscale,\n",
    "    'gauss_blur': gauss_blur,\n",
    "    'resized_crop': resized_crop\n",
    "}\n",
    "\n",
    "transform_widget = widgets.SelectMultiple(\n",
    "    options=transform_dict.keys(),\n",
    "    value=['resized_crop', 'color_jitter', 'grayscale', 'gauss_blur'],\n",
    "    description='Data Transforms',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "transform_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('resized_crop', 'color_jitter', 'grayscale', 'gauss_blur')\n"
     ]
    }
   ],
   "source": [
    "# fetch widget values and close widget\n",
    "selected_transforms = transform_widget.value\n",
    "print(selected_transforms)\n",
    "transform_widget.close()\n",
    "\n",
    "# training transforms for local views\n",
    "transf_list = [transform_dict[transf] for transf in selected_transforms]\n",
    "train_transform = transforms.Compose(\n",
    "    transf_list + [transforms.ToTensor(), normalize])\n",
    "\n",
    "# transform for global view\n",
    "eval_transform = transforms.Compose([transforms.Resize((int(cifar100_dim*0.1 + cifar100_dim))),\n",
    "                                    transforms.CenterCrop(cifar100_dim),\n",
    "                                    transforms.ToTensor(), normalize])\n",
    "\n",
    "\n",
    "def contrastive_transform(sample):\n",
    "    x_i = train_transform(sample)\n",
    "    x_j = train_transform(sample)\n",
    "\n",
    "    return x_i, x_j, eval_transform(sample), transforms.ToTensor()(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_100_train = data_util.CIFAR100CL(root='../data/raw/',\n",
    "                                       train=True,\n",
    "                                       transform=contrastive_transform,\n",
    "                                       download=True\n",
    "                                       )\n",
    "\n",
    "cifar_100_test = data_util.CIFAR100CL(root='../data/raw/',\n",
    "                                      train=False,\n",
    "                                      transform=contrastive_transform,\n",
    "                                      download=True\n",
    "                                      )\n",
    "\n",
    "train_iter = iter(cifar_100_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im, idx, lbl = next(train_iter)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "fig.suptitle(cifar_100_train.classes[lbl])\n",
    "for i in range(4):\n",
    "    tmp = im[i].transpose(0, 2).transpose(0, 1)\n",
    "    axs[i].imshow(tmp)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimCLR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "  <img src=\"./media/cl_overview.gif\" width=\"315\" />\n",
    "  <img src=\"./media/simclr_overview.png\" width=\"451\" /> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we go about \"maximizing agreement\"?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./media/agreement_office.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Measure of similarity $\\text{sim}(.,.)$: cosine similarity\n",
    "  **$\\mathrm{sim}(\\bm u,\\bm v) = \\bm u^\\top \\bm v / \\lVert\\bm u\\rVert \\lVert\\bm v\\rVert$**\n",
    "\n",
    "[//]: # \"l2 norm applied for cosine similarity - as opposed to in dot product - is form of regularization and thus might help restrict network. SimCLR results show that using dot product improves CL task performance but degrades transfer performance -> overfitting\"\n",
    "\n",
    "- Loss to be minimized:\n",
    "  #### $\\mathcal{L}_{(i,j)} = - \\log\\frac{\\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j) / \\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k \\neq i]} \\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_k) / \\tau)}$\n",
    "- Toy example for a batch of 2 examples (part of [this excellent blog post](https://medium.datadriveninvestor.com/simclr-part-2-the-encoder-projection-head-and-loss-function-809a64f30d4a)):\n",
    "  - Transform and encode both examples to obtain $z_1, z_2, z_3, z_4$\n",
    "  - Let's focus on just $z_1$ and calculate all pairwise similarities:\n",
    "    - $a = \\mathrm{sim}(\\bm z_1,\\bm z_2)$\n",
    "    - $b = \\mathrm{sim}(\\bm z_1,\\bm z_3)$\n",
    "    - $c = \\mathrm{sim}(\\bm z_1,\\bm z_4)$\n",
    "  - In order to maximize a while minimizing b and c, and easy loss could be:\n",
    "    ### $\\frac{b+c}{a}$ â†’ $\\frac{e^{b}+e^{c}}{e^{a}}$ â†’ $1 + \\frac{e^{b}+e^{c}}{e^{a}}$ â†’ $\\frac{e^{a}+e^{b}+e^{c}}{e^{a}}$\n",
    "  - And now to arrive at the cross-entropy part:\n",
    "    ### $\\log(\\frac{e^{a}+e^{b}+e^{c}}{e^{a}})$ â†’ $\\log((\\frac{e^{a}}{e^{a}+e^{b}+e^{c}})^{-1})$ â†’ $-\\log(\\frac{e^{a}}{e^{a}+e^{b}+e^{c}})$\n",
    "  - Add temperature scaling and you have the NT-XEnt loss for one of the embeddings of a single example:\n",
    "    ### $-\\log(\\frac{e^\\frac{{a}}{t}}{e^\\frac{{a}}{t}+e^\\frac{{b}}{t}+e^\\frac{{c}}{t}})$\n",
    "- Uniquely desirable property of this loss:\n",
    "  - Implicitly weighs \"hard negatives\" - negatives with high similarity to the anchor - highly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch-lightning wrapper\n",
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import Tensor, nn\n",
    "import torch.utils.data as data\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pl_bolts.optimizers.lars import LARS\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from src.backend import model_zoo\n",
    "\n",
    "\n",
    "# For multi-gpu training\n",
    "class SyncFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor):\n",
    "        ctx.batch_size = tensor.shape[0]\n",
    "\n",
    "        gathered_tensor = [torch.zeros_like(tensor) for _ in range(torch.distributed.get_world_size())]\n",
    "\n",
    "        torch.distributed.all_gather(gathered_tensor, tensor)\n",
    "        gathered_tensor = torch.cat(gathered_tensor, 0)\n",
    "\n",
    "        return gathered_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        torch.distributed.all_reduce(grad_input, op=torch.distributed.ReduceOp.SUM, async_op=False)\n",
    "\n",
    "        idx_from = torch.distributed.get_rank() * ctx.batch_size\n",
    "        idx_to = (torch.distributed.get_rank() + 1) * ctx.batch_size\n",
    "        return grad_input[idx_from:idx_to]\n",
    "\n",
    "\n",
    "# ReLU projection head\n",
    "class NonlinProjection(nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=2048, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.normalize(x, dim=1)\n",
    "\n",
    "\n",
    "# main framework\n",
    "class SimCLR(LightningModule):\n",
    "    def __init__(self,\n",
    "                 hidden_dim=2048,\n",
    "                 proj_dim=128,\n",
    "                 lr=1e-3,\n",
    "                 temperature=0.1,\n",
    "                 weight_decay=1e-6,\n",
    "                 max_epochs=500,\n",
    "                 encoder=None,\n",
    "                 optim='lars'):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        try:\n",
    "            assert self.hparams.temperature > 0.\n",
    "        except AssertionError:\n",
    "            exit('Temperature must be > 0.0.')\n",
    "\n",
    "        # TODO: implement more robust/ dynamic way of specifying encoder and hidden_dim\n",
    "        if encoder:\n",
    "            self.encoder = encoder\n",
    "        else:\n",
    "            self.encoder = model_zoo.resnet18(width=0.5)\n",
    "            self.hparams['hidden_dim'] = 2048\n",
    "\n",
    "        self.head = NonlinProjection(\n",
    "            input_dim=self.hparams.hidden_dim, output_dim=self.hparams.proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def shared_step(self, batch, mode='train'):\n",
    "        # 3rd image for online eval, 4th unprocessed image\n",
    "        # idx is for memory bank or momemntum encoder, irrelevant here\n",
    "        (img1, img2, _, raw), idx, y = batch\n",
    "\n",
    "        if mode != 'train':\n",
    "            # embed the more lightly augmented image\n",
    "            h_val = self(_)\n",
    "            z_val = self.head(h_val)\n",
    "            \n",
    "            self.logger.experiment.add_embedding(z_val, metadata=y.tolist(), label_img=raw, tag='Val Projection')\n",
    "\n",
    "        # get h embeddings\n",
    "        h1 = self(img1)\n",
    "        h2 = self(img2)\n",
    "\n",
    "        # get z projections\n",
    "        z1 = self.head(h1)\n",
    "        z2 = self.head(h2)\n",
    "\n",
    "        loss = self.nt_xent_loss(z1, z2, self.hparams.temperature, mode=mode)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, mode='train')\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch, mode='valid')\n",
    "\n",
    "        self.log('valid_loss', loss, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # paper default\n",
    "        if self.hparams.optim == 'lars':\n",
    "            optimizer = LARS(\n",
    "                self.parameters(),\n",
    "                lr=self.hparams.lr,\n",
    "                momentum=0.9,\n",
    "                weight_decay=self.hparams.weight_decay,\n",
    "                trust_coefficient=0.001,\n",
    "            )\n",
    "        # vanilla adam\n",
    "        elif self.hparams.optim == 'adam':\n",
    "            optimizer = torch.optim.Adam(\n",
    "                self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        # adam with momentum\n",
    "        elif self.hparams.optim == 'adamw':\n",
    "            optimizer = optim.AdamW(\n",
    "                self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f'Optimizer {self.hparams.optim} not supported.')\n",
    "\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.hparams.max_epochs, eta_min=self.hparams.lr / 50\n",
    "        )\n",
    "\n",
    "        scheduler = {\n",
    "            \"scheduler\": lr_scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def nt_xent_loss(self, out_1, out_2, temperature, eps=1e-6, mode='train'):\n",
    "        \"\"\"\n",
    "        assume out_1 and out_2 are l2 normalized\n",
    "        out_1: [batch_size, dim]\n",
    "        out_2: [batch_size, dim]\n",
    "        \"\"\"\n",
    "        # gather representations in case of distributed training\n",
    "        # out_1_dist: [batch_size * world_size, dim]\n",
    "        # out_2_dist: [batch_size * world_size, dim]\n",
    "        if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "            out_1_dist = SyncFunction.apply(out_1)\n",
    "            out_2_dist = SyncFunction.apply(out_2)\n",
    "        else:\n",
    "            out_1_dist = out_1\n",
    "            out_2_dist = out_2\n",
    "\n",
    "        # out: [2 * batch_size, dim]\n",
    "        # out_dist: [2 * batch_size * world_size, dim]\n",
    "        out = torch.cat([out_1, out_2], dim=0)\n",
    "        out_dist = torch.cat([out_1_dist, out_2_dist], dim=0)\n",
    "\n",
    "        # cov and sim: [2 * batch_size, 2 * batch_size]\n",
    "        # neg: [2 * batch_size]\n",
    "        cov = torch.mm(out, out_dist.t())\n",
    "        sim = torch.exp(cov / temperature)\n",
    "\n",
    "        # loss denominator\n",
    "        neg = sim.sum(dim=-1)\n",
    "        # from each row, subtract e^(1/temp) to remove similarity measure for x1.x1 -> easier than masking\n",
    "        row_sub = Tensor(neg.shape).fill_(\n",
    "            math.e ** (1 / temperature)).to(neg.device)\n",
    "        # clamp for numerical stability\n",
    "        neg = torch.clamp(neg - row_sub, min=eps)\n",
    "\n",
    "        # loss numerator\n",
    "        # Positive similarity, pos becomes [2 * batch_size]\n",
    "        pos = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
    "        pos = torch.cat([pos, pos], dim=0)\n",
    "\n",
    "        loss = -torch.log(pos / (neg + eps)).mean()\n",
    "\n",
    "        # calculate accuracy metrics\n",
    "        # copy similarity matrix, no gradients needed for this part so detach\n",
    "        sim_eval = sim.detach()\n",
    "        # first find values to mask where similarity was calculated with itself& zero out\n",
    "        self_mask = torch.eye(sim_eval.shape[0], dtype=torch.bool, device=sim_eval.device)\n",
    "        sim_eval = sim_eval.masked_fill(self_mask, -9e15)\n",
    "        # Find positive example -> batch_size//2 away from the original example\n",
    "        pos_mask = self_mask.roll(shifts=sim_eval.shape[0] // 2, dims=0)\n",
    "        # Get ranking position of positive example by logit score\n",
    "        comb_sim = torch.cat(\n",
    "            [sim_eval[pos_mask][:, None], sim_eval.masked_fill(pos_mask, -9e15)],  # First position positive example\n",
    "            dim=-1,\n",
    "        )\n",
    "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "        # Logging ranking metrics\n",
    "        # positive sample ranked first\n",
    "        self.log(mode + \"_acc_top1\", (sim_argsort == 0).float().mean(), on_step=True, on_epoch=False)\n",
    "        # positive sample ranked in top 5 \n",
    "        self.log(mode + \"_acc_top5\", (sim_argsort < 5).float().mean(), on_step=True, on_epoch=False)\n",
    "        # mean position of positive sample among entire batch\n",
    "        self.log(mode + \"_acc_mean_pos\", 1 + sim_argsort.float().mean(), on_step=True, on_epoch=False)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simclr_trainer(train_ds, val_ds, batch_size=128, max_epochs=500, **kwargs):\n",
    "    tb_logger = pl_loggers.TensorBoardLogger(save_dir='../results/simclr_demo/lightning_logs')\n",
    "    trainer = Trainer(\n",
    "        default_root_dir='../results/simclr_demo/model_checkpoints',\n",
    "        gpus=0,\n",
    "        max_epochs=500,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"train_loss\"),\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],\n",
    "        progress_bar_refresh_rate=1,\n",
    "        logger=tb_logger,\n",
    "        log_every_n_steps=2\n",
    "    )\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        drop_last=True)\n",
    "\n",
    "    valid_loader = data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        drop_last=True)\n",
    "\n",
    "    pl.seed_everything(13)\n",
    "\n",
    "    model = SimCLR(max_epochs=max_epochs, **kwargs)\n",
    "    trainer.fit(model, train_loader, valid_loader)\n",
    "    \n",
    "    # load& return best model after training is complete\n",
    "    model = SimCLR.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Global seed set to 13\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | encoder | ResNet           | 116 M \n",
      "1 | head    | NonlinProjection | 4.5 M \n",
      "---------------------------------------------\n",
      "121 M     Trainable params\n",
      "0         Non-trainable params\n",
      "121 M     Total params\n",
      "484.264   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5d0e1bd7204b4cafacfac03faf9cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n",
      "/Users/yannikglaser/miniforge3/envs/ssl_demo/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56b2a5139fc4fa98c2918953e163b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simclr_model = simclr_trainer(cifar_100_train, cifar_100_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at some of the observations from the paper:\n",
    "### Augmentations matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./media/augmentation_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./media/cl_views.png)\n",
    "\n",
    "[//]: # \"From the paper https://arxiv.org/abs/2005.10243\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive learning models are superb semi-supervised and transfer learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./media/simclr_transfer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./media/semisup_table7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./media/semisup_simclrv2.png)\n",
    "\n",
    "[//]: # \"From the paper https://arxiv.org/abs/2006.10029\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, CL is expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./media/training_cost.png)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1fece7c4f4adb239025c48c3ec00e743632c1c3caa6a6cb39a39f7bcaab8c3d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ssl_demo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
